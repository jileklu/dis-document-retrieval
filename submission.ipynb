{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9715678,"sourceType":"datasetVersion","datasetId":5943616},{"sourceId":9774702,"sourceType":"datasetVersion","datasetId":5987644},{"sourceId":9806090,"sourceType":"datasetVersion","datasetId":6010705}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport math\nimport heapq\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom scipy.sparse import csr_matrix, vstack\nfrom concurrent.futures import ProcessPoolExecutor\nimport os\nimport time\nfrom datetime import datetime\n\n# Global variables accessible by worker processes\nGLOBAL_IDF_DICT = {}\nGLOBAL_TERM_TO_INDEX = {}\n\ndef init_worker(idf_dict, term_to_index):\n    global GLOBAL_IDF_DICT\n    global GLOBAL_TERM_TO_INDEX\n    GLOBAL_IDF_DICT = idf_dict\n    GLOBAL_TERM_TO_INDEX = term_to_index\n\n# Helper function to print messages with timestamps\ndef log(message):\n    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} {message}\")\n\ndef compute_smoothed_tf(tokens):\n    \"\"\"Compute term frequency with log scaling for each token in the list of tokens.\"\"\"\n    tf_dict = {}\n    \n    for term in tokens:\n        tf_dict[term] = tf_dict.get(term, 0) + 1\n    # Apply log scaling and smoothing\n    for term in tf_dict:\n        tf_dict[term] = 1 + math.log(tf_dict[term])  # Smoothing\n    return tf_dict\n\ndef compute_smoothed_idf(corpus_terms_set, num_docs):\n    \"\"\"Compute smoothed inverse document frequency for terms across the entire corpus.\"\"\"\n    idf_dict = {}\n    for term, doc_freq in corpus_terms_set.items():\n        # Adjusted IDF calculation to match sklearn's implementation\n        idf_dict[term] = math.log((1 + num_docs) / (1 + doc_freq)) + 1\n    return idf_dict\n\ndef compute_tf_idf_vector(tokens, idf_dict, term_to_index):\n    \"\"\"Compute normalized TF-IDF vector and store it as a sparse vector.\"\"\"\n    tf = compute_smoothed_tf(tokens)\n    indices = []\n    data = []\n    for term, tf_value in tf.items():\n        if term in term_to_index:\n            idx = term_to_index[term]\n            idf_value = idf_dict.get(term, 0)\n            tf_idf_value = tf_value * idf_value\n            indices.append(idx)\n            data.append(tf_idf_value)\n    # Create sparse vector\n    vector_size = len(term_to_index)\n    tf_idf_vector = csr_matrix((data, indices, [0, len(indices)]), shape=(1, vector_size))\n    # Normalize the vector\n    norm = np.linalg.norm(tf_idf_vector.data)\n    if norm > 0:\n        tf_idf_vector.data = tf_idf_vector.data / norm\n    return tf_idf_vector\n\ndef calculate_idf_from_corpus(corpus_path, vocab_output_path='vocab.json'):\n    \"\"\"First pass over the corpus to calculate document frequencies for each term.\"\"\"\n    log(\"Starting IDF calculation from corpus...\")\n    start_time = time.time()\n    corpus_terms_set = {}\n    num_docs = 0\n    term_to_index = {}\n    index = 0\n    with open(corpus_path, 'r') as f:\n        for idx, line in enumerate(f):\n            doc = json.loads(line)\n            tokens = set(doc['tokens'])\n            num_docs += 1\n            for term in tokens:\n                if term not in corpus_terms_set:\n                    corpus_terms_set[term] = 1\n                    term_to_index[term] = index\n                    index += 1\n                else:\n                    corpus_terms_set[term] += 1\n            if idx % 50000 == 0 and idx > 0:\n                log(f\"Processed {idx} documents...\")\n    idf_dict = compute_smoothed_idf(corpus_terms_set, num_docs)\n    log(\"IDF calculation complete.\")\n    # Save the vocabulary\n    #with open(vocab_output_path, 'w') as vocab_file:\n    #    json.dump(term_to_index, vocab_file)\n    #end_time = time.time()\n    #log(f\"IDF calculation took {end_time - start_time:.2f} seconds.\")\n    return idf_dict, term_to_index\n\ndef process_document_tf_idf(line, idf_dict, term_to_index):\n    \"\"\"Process a single document to compute its TF-IDF vector.\"\"\"\n    doc = json.loads(line)\n    doc_id = doc['doc_id']\n    tokens = doc['tokens']\n    tf_idf_vector = compute_tf_idf_vector(tokens, idf_dict, term_to_index)\n    # Save the vector as indices and data\n    indices = tf_idf_vector.indices.tolist()\n    data = tf_idf_vector.data.tolist()\n    return json.dumps({doc_id: {\"indices\": indices, \"data\": data}})\n\ndef wrapper_process_document_tf_idf(args):\n    \"\"\"Wrapper function to unpack arguments for multiprocessing.\"\"\"\n    return process_document_tf_idf(*args)\n\ndef compute_tf_idf_for_documents(corpus_path, idf_dict, term_to_index, output_path='doc_tf_idf_vectors.json'):\n    log(\"Starting TF-IDF computation for documents...\")\n    start_time = time.time()\n    with open(output_path, 'w') as output_file, open(corpus_path, 'r') as f:\n        total_docs = sum(1 for _ in f)  # Count total documents\n        f.seek(0)  # Reset file pointer to beginning\n        log(f\"Total documents to process: {total_docs}\")\n        with ProcessPoolExecutor() as executor:\n            # Use a generator to avoid loading all lines into memory\n            args = ((line, idf_dict, term_to_index) for line in f)\n            results = executor.map(wrapper_process_document_tf_idf, args, chunksize=1000)\n            for idx, result in enumerate(results, 1):\n                output_file.write(result + '\\n')\n                if idx % 50000 == 0:\n                    log(f\"Processed {idx}/{total_docs} documents.\")\n    end_time = time.time()\n    log(f\"TF-IDF computation for documents complete. Took {end_time - start_time:.2f} seconds.\")\n    log(f\"TF-IDF vectors saved to {output_path}.\")\n\ndef process_query_tf_idf(line):\n    \"\"\"Process a single query to compute its TF-IDF vector using global IDF and term mappings.\"\"\"\n    query = json.loads(line)\n    query_id = query['query_id']\n    tokens = query['tokens']\n    tf_idf_vector = compute_tf_idf_vector(tokens, GLOBAL_IDF_DICT, GLOBAL_TERM_TO_INDEX)\n    # Save the vector as indices and data\n    indices = tf_idf_vector.indices.tolist()\n    data = tf_idf_vector.data.tolist()\n    return json.dumps({query_id: {\"indices\": indices, \"data\": data}})\n\ndef wrapper_process_query_tf_idf(line):\n    \"\"\"Wrapper function to process a query line.\"\"\"\n    return process_query_tf_idf(line)\n\ndef compute_tf_idf_for_queries(queries_path, idf_dict, term_to_index, output_path='query_tf_idf_vectors.json'):\n    \"\"\"Compute TF-IDF vectors for all queries using multiprocessing with shared IDF and term mappings.\"\"\"\n    log(\"Starting TF-IDF computation for queries...\")\n    start_time = time.time()\n    with open(output_path, 'w') as output_file, open(queries_path, 'r') as f:\n        # Generator to read lines one by one\n        lines = f\n        total_queries = sum(1 for _ in f)\n        f.seek(0)  # Reset file pointer after counting\n        log(f\"Total queries to process: {total_queries}\")\n        with ProcessPoolExecutor(initializer=init_worker, initargs=(idf_dict, term_to_index)) as executor:\n            # Process queries with a reasonable chunksize\n            results = executor.map(wrapper_process_query_tf_idf, lines, chunksize=100)\n            for idx, result in enumerate(results, 1):\n                output_file.write(result + '\\n')\n                if idx % 100 == 0 or idx == total_queries:\n                    log(f\"Processed {idx}/{total_queries} queries.\")\n    end_time = time.time()\n    log(f\"TF-IDF computation for queries complete. Took {end_time - start_time:.2f} seconds.\")\n    log(f\"TF-IDF vectors saved to {output_path}.\")\n\ndef build_inverted_index_on_disk(doc_tf_idf_path, output_index_path='inverted_index.json'):\n    \"\"\"Build an inverted index from the document TF-IDF vectors stored in JSON format and save it to disk.\"\"\"\n    log(\"Starting inverted index construction...\")\n    start_time = time.time()\n    chunk_size = 50000  # Number of documents to process per chunk\n    temp_index_files = []\n    term_to_doc = defaultdict(set)\n    idx = 0\n    chunk_idx = 0\n    with open(doc_tf_idf_path, 'r') as f:\n        for line in f:\n            doc_entry = json.loads(line)\n            doc_id = list(doc_entry.keys())[0]\n            indices = doc_entry[doc_id][\"indices\"]\n            # For each term index, add document ID to term's set\n            for term_idx in indices:\n                term_to_doc[term_idx].add(doc_id)\n            idx += 1\n            if idx % chunk_size == 0:\n                # Write the partial inverted index to a temporary file\n                temp_file_path = f\"temp_index_{chunk_idx}.json\"\n                with open(temp_file_path, 'w') as temp_file:\n                    for term_idx, doc_ids in term_to_doc.items():\n                        temp_file.write(json.dumps({term_idx: list(doc_ids)}) + '\\n')\n                        temp_index_files.append(temp_file_path)\n                        term_to_doc.clear()\n                        chunk_idx += 1\n                        log(f\"Processed {idx} documents. Partial inverted index saved to {temp_file_path}.\")\n    # Write any remaining terms\n    if term_to_doc:\n        temp_file_path = f\"temp_index_{chunk_idx}.json\"\n        with open(temp_file_path, 'w') as temp_file:\n            for term_idx, doc_ids in term_to_doc.items():\n                temp_file.write(json.dumps({term_idx: list(doc_ids)}) + '\\n')\n        temp_index_files.append(temp_file_path)\n        term_to_doc.clear()\n        log(f\"Processed {idx} documents. Final partial inverted index saved to {temp_file_path}.\")\n    # Now merge the partial inverted indexes\n    log(\"Merging partial inverted indexes...\")\n    term_to_doc_merged = defaultdict(set)\n    for temp_file_path in temp_index_files:\n        with open(temp_file_path, 'r') as temp_file:\n            for line in temp_file:\n                term_entry = json.loads(line)\n                term_idx = list(term_entry.keys())[0]\n                doc_ids = term_entry[term_idx]\n                term_to_doc_merged[term_idx].update(doc_ids)\n        # Remove temporary file to save space\n        os.remove(temp_file_path)\n        term_to_doc.clear()\n        log(f\"Merged and removed temporary file {temp_file_path}.\")\n\n    # Write the final inverted index\n    with open(output_index_path, 'w') as output_file:\n        for term_idx, doc_ids in term_to_doc_merged.items():\n            output_file.write(json.dumps({term_idx: list(doc_ids)}) + '\\n')\n\n    end_time = time.time()\n    log(\"Inverted index building complete and saved to disk.\")\n    log(f\"Inverted index saved to {output_index_path}.\")\n    log(f\"Inverted index construction took {end_time - start_time:.2f} seconds.\")\n\ndef build_term_to_queries_mapping_top_terms(queries, K):\n    \"\"\"\n    Build a mapping from term index to the list of query IDs that contain the term,\n    considering only the top K terms per query.\n    \"\"\"\n    term_to_queries = defaultdict(list)\n    query_top_terms = {}\n    for query_id, query_vector in queries.items():\n        indices = query_vector.indices\n        data = query_vector.data\n        if len(data) > K:\n            top_indices = np.argsort(data)[-K:]\n            top_terms = indices[top_indices]\n        else:\n            top_terms = indices\n        query_top_terms[query_id] = set(top_terms)\n        for term_idx in top_terms:\n            term_to_queries[term_idx].append(query_id)\n    return term_to_queries, query_top_terms\n\ndef retrieve_top_documents_with_limited_candidates(query_tf_idf_path, doc_tf_idf_path, index_path, term_to_index_path, top_n=10, K=5, M=200):\n    \"\"\"Retrieve top N documents for each query using the inverted index with limited candidates.\"\"\"\n    log(\"Starting retrieval of top documents for each query...\")\n    start_time = time.time()\n\n    # Load term_to_index mapping\n    with open(term_to_index_path, 'r') as f:\n        term_to_index = json.load(f)\n    log(\"Term to index mapping loaded.\")\n\n    # Load query vectors and get top K terms\n    queries = {}\n    query_ids_list = []\n    with open(query_tf_idf_path, 'r') as f:\n        for line in f:\n            query_entry = json.loads(line)\n            query_id = list(query_entry.keys())[0]\n            data = query_entry[query_id]\n            indices = np.array(data['indices'])\n            values = np.array(data['data'])\n            vector_size = len(term_to_index)\n            query_vector = csr_matrix((values, indices, [0, len(indices)]), shape=(1, vector_size))\n            queries[query_id] = query_vector\n            query_ids_list.append(query_id)\n    log(\"Query vectors loaded.\")\n\n    # Build term_to_queries mapping considering top K terms per query\n    term_to_queries, query_top_terms = build_term_to_queries_mapping_top_terms(queries, K)\n    log(\"Term to queries mapping for top terms created.\")\n\n    # Initialize candidate document lists\n    query_candidates = defaultdict(set)\n    all_candidate_doc_ids = set()\n\n    # Iterate through the inverted index and assign documents to queries\n    with open(index_path, 'r') as index_file:\n        for idx, line in enumerate(index_file, 1):\n            term_entry = json.loads(line)\n            term_idx_str = list(term_entry.keys())[0]\n            doc_ids = term_entry[term_idx_str]\n            term_idx = int(term_idx_str)\n\n            # Retrieve queries that contain this term in their top K terms\n            relevant_queries = term_to_queries.get(term_idx, [])\n            if relevant_queries:\n                # Limit the number of documents per term to M\n                limited_doc_ids = doc_ids[:M]\n                for query_id in relevant_queries:\n                    query_candidates[query_id].update(limited_doc_ids)\n                    all_candidate_doc_ids.update(limited_doc_ids)\n\n            # Optional: Log progress every 1,000,000 terms\n            if idx % 1000000 == 0:\n                log(f\"Processed {idx} terms from the inverted index.\")\n\n    log(\"Candidate documents collected for queries.\")\n\n    # Build doc_vectors mapping for candidate documents\n    doc_vectors = {}\n    with open(doc_tf_idf_path, 'r') as doc_file:\n        for line in doc_file:\n            doc_entry = json.loads(line)\n            doc_id = list(doc_entry.keys())[0]\n            if doc_id in all_candidate_doc_ids:\n                data = doc_entry[doc_id]\n                indices = data['indices']\n                values = data['data']\n                vector_size = len(term_to_index)\n                doc_vector = csr_matrix((values, indices, [0, len(indices)]), shape=(1, vector_size))\n                doc_vectors[doc_id] = doc_vector\n    log(\"Document vectors for candidates loaded.\")\n\n    # Now computing similarities for each query...\n    submission_data = []\n    total_queries = len(query_ids_list)\n\n    for idx, query_id in enumerate(query_ids_list):\n        candidate_doc_ids = query_candidates.get(query_id, set())\n        query_vector = queries[query_id]\n        result = process_single_query(query_id, query_vector, candidate_doc_ids, doc_vectors, top_n)\n        submission_data.append({\n            'id': idx,\n            'docids': result['docids']\n        })\n        if (idx + 1) % 100 == 0 or (idx + 1) == total_queries:\n            log(f\"Processed {idx + 1}/{total_queries} queries.\")\n\n    log(\"Top documents retrieved for all queries.\")\n    end_time = time.time()\n    log(f\"Retrieval completed in {end_time - start_time:.2f} seconds.\")\n    return submission_data\n\ndef process_single_query(query_id, query_vector, candidate_doc_ids, doc_vectors, top_n):\n    candidate_doc_ids_list = list(candidate_doc_ids)\n    docs = []\n    doc_ids_list = []\n    for doc_id in candidate_doc_ids_list:\n        doc_vector = doc_vectors.get(doc_id)\n        if doc_vector is not None:\n            docs.append(doc_vector)\n            doc_ids_list.append(doc_id)\n    if not docs:\n        retrieved_doc_ids = []\n    else:\n        doc_matrix = vstack(docs)\n        similarities = doc_matrix.dot(query_vector.T).toarray().flatten()\n        # Use argpartition to get indices of top N similarities\n        top_n = min(top_n, len(similarities))\n        top_n_indices = np.argpartition(-similarities, top_n - 1)[:top_n]\n        top_docs = [(similarities[i], doc_ids_list[i]) for i in top_n_indices]\n        # Sort the top documents by similarity\n        top_docs.sort(reverse=True)\n        retrieved_doc_ids = [doc_id for sim, doc_id in top_docs]\n    formatted_doc_ids = str(retrieved_doc_ids)\n    return {'docids': formatted_doc_ids}\n\n# Example usage\n\ndef load_vocab(vocab_path):\n    \"\"\"\n    Load the vocabulary from a JSON file.\n\n    Args:\n        vocab_path (str): Path to the vocabulary JSON file.\n\n    Returns:\n        dict: The term-to-index mapping.\n    \"\"\"\n    with open(vocab_path, 'r') as f:\n        term_to_index = json.load(f)\n    return term_to_index\n\nif __name__ == \"__main__\":\n    # Paths to the TF-IDF vector files\n    corpus_path = '/kaggle/input/dis-tokens/corpus_tokens.json'\n    queries_path = '/kaggle/input/dis-tokens/test_tokens.json'\n    doc_tf_idf_path = '/kaggle/input/tf-idf/doc_tf_idf_vectors.json'\n    query_tf_idf_path = 'query_tf_idf_vectors.json'\n    index_path = '/kaggle/input/tf-idf/inverted_index.json'\n    term_to_index_path = '/kaggle/input/tf-idf/vocab.json'\n\n    # Step 1: Calculate IDF using the entire corpus\n    # Uncomment if running for the first time\n    idf_dict, term_to_index = calculate_idf_from_corpus(corpus_path, term_to_index_path)\n\n    term_to_index = load_vocab(term_to_index_path)\n\n    # Step 2: Compute TF-IDF vectors for all documents\n    # Uncomment if running for the first time\n    # compute_tf_idf_for_documents(corpus_path, idf_dict, term_to_index, doc_tf_idf_path)\n\n    # Step 3: Compute TF-IDF vectors for all queries\n    # Uncomment if running for the first time\n    compute_tf_idf_for_queries(queries_path, idf_dict, term_to_index, query_tf_idf_path)\n\n    # Step 4: Build the inverted index and save it to disk incrementally\n    # Uncomment if running for the first time\n    # build_inverted_index_on_disk(doc_tf_idf_path, index_path)\n\n    # Step 5: Retrieve top documents using the optimized inverted index with limited candidates\n    submission_data = retrieve_top_documents_with_limited_candidates(\n        query_tf_idf_path,\n        doc_tf_idf_path,\n        index_path,\n        term_to_index_path,\n        top_n=10,\n        K=30,   # Top K terms per query\n        M=2000  # Top M documents per term\n    )\n\n    # Step 6: Convert to DataFrame and save to CSV\n    submission_df = pd.DataFrame(submission_data)\n    submission_df = submission_df[['id', 'docids']]  # Ensure correct column order\n\n    # Verify that there are 2000 lines plus header\n    assert len(submission_df) == 2000, f\"Expected 2000 queries, but got {len(submission_df)}\"\n\n    submission_file_path = 'submission.csv'\n    submission_df.to_csv(submission_file_path, index=False)\n\n    log(f\"Submission file '{submission_file_path}' created successfully.\")\n    log(submission_df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-09T20:35:59.316699Z","iopub.execute_input":"2024-11-09T20:35:59.317357Z","iopub.status.idle":"2024-11-09T21:07:02.733966Z","shell.execute_reply.started":"2024-11-09T20:35:59.317300Z","shell.execute_reply":"2024-11-09T21:07:02.732520Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2024-11-09 20:35:59 Starting IDF calculation from corpus...\n2024-11-09 20:36:53 Processed 50000 documents...\n2024-11-09 20:37:40 Processed 100000 documents...\n2024-11-09 20:38:27 Processed 150000 documents...\n2024-11-09 20:39:14 Processed 200000 documents...\n2024-11-09 20:42:16 Processed 250000 documents...\n2024-11-09 20:43:29 IDF calculation complete.\n2024-11-09 20:43:49 Starting TF-IDF computation for queries...\n2024-11-09 20:43:49 Total queries to process: 2000\n2024-11-09 20:43:49 Processed 100/2000 queries.\n2024-11-09 20:43:49 Processed 200/2000 queries.\n2024-11-09 20:43:49 Processed 300/2000 queries.\n2024-11-09 20:43:49 Processed 400/2000 queries.\n2024-11-09 20:43:49 Processed 500/2000 queries.\n2024-11-09 20:43:49 Processed 600/2000 queries.\n2024-11-09 20:43:49 Processed 700/2000 queries.\n2024-11-09 20:43:49 Processed 800/2000 queries.\n2024-11-09 20:43:49 Processed 900/2000 queries.\n2024-11-09 20:43:49 Processed 1000/2000 queries.\n2024-11-09 20:43:49 Processed 1100/2000 queries.\n2024-11-09 20:43:49 Processed 1200/2000 queries.\n2024-11-09 20:43:49 Processed 1300/2000 queries.\n2024-11-09 20:43:49 Processed 1400/2000 queries.\n2024-11-09 20:43:49 Processed 1500/2000 queries.\n2024-11-09 20:43:49 Processed 1600/2000 queries.\n2024-11-09 20:43:49 Processed 1700/2000 queries.\n2024-11-09 20:43:49 Processed 1800/2000 queries.\n2024-11-09 20:43:49 Processed 1900/2000 queries.\n2024-11-09 20:43:49 Processed 2000/2000 queries.\n2024-11-09 20:43:50 TF-IDF computation for queries complete. Took 0.95 seconds.\n2024-11-09 20:43:50 TF-IDF vectors saved to query_tf_idf_vectors.json.\n2024-11-09 20:43:50 Starting retrieval of top documents for each query...\n2024-11-09 20:44:06 Term to index mapping loaded.\n2024-11-09 20:44:07 Query vectors loaded.\n2024-11-09 20:44:07 Term to queries mapping for top terms created.\n2024-11-09 20:45:13 Processed 1000000 terms from the inverted index.\n2024-11-09 20:45:22 Processed 2000000 terms from the inverted index.\n2024-11-09 20:45:29 Processed 3000000 terms from the inverted index.\n2024-11-09 20:45:37 Processed 4000000 terms from the inverted index.\n2024-11-09 20:45:47 Processed 5000000 terms from the inverted index.\n2024-11-09 20:45:54 Processed 6000000 terms from the inverted index.\n2024-11-09 20:46:02 Processed 7000000 terms from the inverted index.\n2024-11-09 20:46:11 Processed 8000000 terms from the inverted index.\n2024-11-09 20:46:18 Processed 9000000 terms from the inverted index.\n2024-11-09 20:46:27 Processed 10000000 terms from the inverted index.\n2024-11-09 20:46:33 Processed 11000000 terms from the inverted index.\n2024-11-09 20:46:34 Candidate documents collected for queries.\n2024-11-09 20:51:39 Document vectors for candidates loaded.\n2024-11-09 20:52:18 Processed 100/2000 queries.\n2024-11-09 20:53:00 Processed 200/2000 queries.\n2024-11-09 20:53:40 Processed 300/2000 queries.\n2024-11-09 20:54:24 Processed 400/2000 queries.\n2024-11-09 20:55:20 Processed 500/2000 queries.\n2024-11-09 20:56:15 Processed 600/2000 queries.\n2024-11-09 20:57:10 Processed 700/2000 queries.\n2024-11-09 20:58:16 Processed 800/2000 queries.\n2024-11-09 20:59:22 Processed 900/2000 queries.\n2024-11-09 21:00:26 Processed 1000/2000 queries.\n2024-11-09 21:00:59 Processed 1100/2000 queries.\n2024-11-09 21:01:31 Processed 1200/2000 queries.\n2024-11-09 21:02:28 Processed 1300/2000 queries.\n2024-11-09 21:03:23 Processed 1400/2000 queries.\n2024-11-09 21:04:11 Processed 1500/2000 queries.\n2024-11-09 21:04:59 Processed 1600/2000 queries.\n2024-11-09 21:05:28 Processed 1700/2000 queries.\n2024-11-09 21:06:00 Processed 1800/2000 queries.\n2024-11-09 21:06:30 Processed 1900/2000 queries.\n2024-11-09 21:07:00 Processed 2000/2000 queries.\n2024-11-09 21:07:00 Top documents retrieved for all queries.\n2024-11-09 21:07:00 Retrieval completed in 1390.05 seconds.\n2024-11-09 21:07:02 Submission file 'submission.csv' created successfully.\n2024-11-09 21:07:02    id                                             docids\n0   0  ['doc-en-329732', 'doc-en-0', 'doc-en-166569',...\n1   1  ['doc-en-243194', 'doc-en-284401', 'doc-en-447...\n2   2  ['doc-en-24920', 'doc-en-369610', 'doc-en-2483...\n3   3  ['doc-en-70780', 'doc-en-814022', 'doc-en-4416...\n4   4  ['doc-en-27626', 'doc-en-520599', 'doc-en-3510...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import json\nimport math\nimport heapq\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom scipy.sparse import csr_matrix, vstack\nimport os\nimport time\nfrom datetime import datetime\n\n# Helper function to print messages with timestamps\ndef log(message):\n    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} {message}\")\n\ndef compute_smoothed_tf(tokens):\n    \"\"\"Compute term frequency with log scaling for each token in the list of tokens.\"\"\"\n    tf_dict = {}\n    for term in tokens:\n        tf_dict[term] = tf_dict.get(term, 0) + 1\n    # Apply log scaling and smoothing\n    for term in tf_dict:\n        tf_dict[term] = 1 + math.log(tf_dict[term])  # Smoothing\n    return tf_dict\n\ndef compute_smoothed_idf(corpus_terms_set, num_docs):\n    \"\"\"Compute smoothed inverse document frequency for terms across the entire corpus.\"\"\"\n    idf_dict = {}\n    for term, doc_freq in corpus_terms_set.items():\n        # Adjusted IDF calculation to match sklearn's implementation\n        idf_dict[term] = math.log((1 + num_docs) / (1 + doc_freq)) + 1\n    return idf_dict\n\ndef compute_tf_idf_vector(tokens, idf_dict, term_to_index):\n    \"\"\"Compute normalized TF-IDF vector and store it as a sparse vector.\"\"\"\n    tf = compute_smoothed_tf(tokens)\n    indices = []\n    data = []\n    for term, tf_value in tf.items():\n        if term in term_to_index:\n            idx = term_to_index[term]\n            idf_value = idf_dict.get(term, 0)\n            tf_idf_value = tf_value * idf_value\n            indices.append(idx)\n            data.append(tf_idf_value)\n    # Create sparse vector\n    vector_size = len(term_to_index)\n    tf_idf_vector = csr_matrix((data, indices, [0, len(indices)]), shape=(1, vector_size))\n    # Normalize the vector\n    norm = np.linalg.norm(tf_idf_vector.data)\n    if norm > 0:\n        tf_idf_vector.data = tf_idf_vector.data / norm\n    return tf_idf_vector\n\ndef build_term_to_queries_mapping_top_terms(queries, K):\n    \"\"\"\n    Build a mapping from term index to the list of query IDs that contain the term,\n    considering only the top K terms per query.\n    \"\"\"\n    term_to_queries = defaultdict(list)\n    query_top_terms = {}\n    for query_id, query_vector in queries.items():\n        indices = query_vector.indices\n        data = query_vector.data\n        if len(data) > K:\n            top_indices = np.argsort(data)[-K:]\n            top_terms = indices[top_indices]\n        else:\n            top_terms = indices\n        query_top_terms[query_id] = set(top_terms)\n        for term_idx in top_terms:\n            term_to_queries[term_idx].append(query_id)\n    return term_to_queries, query_top_terms\n\ndef retrieve_top_documents_with_limited_candidates(query_tf_idf_path, doc_tf_idf_path, index_path, term_to_index_path, top_n=10, K=5, M=200):\n    \"\"\"Retrieve top N documents for each query using the inverted index with limited candidates.\"\"\"\n    log(\"Starting retrieval of top documents for each query...\")\n    start_time = time.time()\n\n    # Load term_to_index mapping\n    with open(term_to_index_path, 'r') as f:\n        term_to_index = json.load(f)\n    log(\"Term to index mapping loaded.\")\n\n    # Load query vectors and get top K terms\n    queries = {}\n    query_ids_list = []\n    with open(query_tf_idf_path, 'r') as f:\n        for line in f:\n            query_entry = json.loads(line)\n            query_id = list(query_entry.keys())[0]\n            data = query_entry[query_id]\n            indices = np.array(data['indices'])\n            values = np.array(data['data'])\n            vector_size = len(term_to_index)\n            query_vector = csr_matrix((values, indices, [0, len(indices)]), shape=(1, vector_size))\n            queries[query_id] = query_vector\n            query_ids_list.append(query_id)\n    log(\"Query vectors loaded.\")\n\n    # Build term_to_queries mapping considering top K terms per query\n    term_to_queries, query_top_terms = build_term_to_queries_mapping_top_terms(queries, K)\n    log(\"Term to queries mapping for top terms created.\")\n\n    # Initialize candidate document lists\n    query_candidates = defaultdict(set)\n    all_candidate_doc_ids = set()\n\n    # Iterate through the inverted index and assign documents to queries\n    with open(index_path, 'r') as index_file:\n        for idx, line in enumerate(index_file, 1):\n            term_entry = json.loads(line)\n            term_idx_str = list(term_entry.keys())[0]\n            doc_ids = term_entry[term_idx_str]\n            term_idx = int(term_idx_str)\n\n            # Retrieve queries that contain this term in their top K terms\n            relevant_queries = term_to_queries.get(term_idx, [])\n            if relevant_queries:\n                # Limit the number of documents per term to M\n                limited_doc_ids = doc_ids[:M]\n                for query_id in relevant_queries:\n                    query_candidates[query_id].update(limited_doc_ids)\n                    all_candidate_doc_ids.update(limited_doc_ids)\n\n            # Optional: Log progress every 1,000,000 terms\n            if idx % 1000000 == 0:\n                log(f\"Processed {idx} terms from the inverted index.\")\n\n    log(\"Candidate documents collected for queries.\")\n\n    # Build doc_vectors mapping for candidate documents\n    doc_vectors = {}\n    with open(doc_tf_idf_path, 'r') as doc_file:\n        for line in doc_file:\n            doc_entry = json.loads(line)\n            doc_id = list(doc_entry.keys())[0]\n            if doc_id in all_candidate_doc_ids:\n                data = doc_entry[doc_id]\n                indices = data['indices']\n                values = data['data']\n                vector_size = len(term_to_index)\n                doc_vector = csr_matrix((values, indices, [0, len(indices)]), shape=(1, vector_size))\n                doc_vectors[doc_id] = doc_vector\n    log(\"Document vectors for candidates loaded.\")\n\n    # Now computing similarities for each query...\n    submission_data = []\n    total_queries = len(query_ids_list)\n\n    for idx, query_id in enumerate(query_ids_list):\n        candidate_doc_ids = query_candidates.get(query_id, set())\n        query_vector = queries[query_id]\n        result = process_single_query(query_id, query_vector, candidate_doc_ids, doc_vectors, top_n)\n        submission_data.append({\n            'id': idx,\n            'docids': result['docids']\n        })\n        if (idx + 1) % 100 == 0 or (idx + 1) == total_queries:\n            log(f\"Processed {idx + 1}/{total_queries} queries.\")\n\n    log(\"Top documents retrieved for all queries.\")\n    end_time = time.time()\n    log(f\"Retrieval completed in {end_time - start_time:.2f} seconds.\")\n    return submission_data\n\ndef process_single_query(query_id, query_vector, candidate_doc_ids, doc_vectors, top_n):\n    candidate_doc_ids_list = list(candidate_doc_ids)\n    docs = []\n    doc_ids_list = []\n    for doc_id in candidate_doc_ids_list:\n        doc_vector = doc_vectors.get(doc_id)\n        if doc_vector is not None:\n            docs.append(doc_vector)\n            doc_ids_list.append(doc_id)\n    if not docs:\n        retrieved_doc_ids = []\n    else:\n        doc_matrix = vstack(docs)\n        similarities = doc_matrix.dot(query_vector.T).toarray().flatten()\n        # Use argpartition to get indices of top N similarities\n        top_n = min(top_n, len(similarities))\n        top_n_indices = np.argpartition(-similarities, top_n - 1)[:top_n]\n        top_docs = [(similarities[i], doc_ids_list[i]) for i in top_n_indices]\n        # Sort the top documents by similarity\n        top_docs.sort(reverse=True)\n        retrieved_doc_ids = [doc_id for sim, doc_id in top_docs]\n    formatted_doc_ids = str(retrieved_doc_ids)\n    return {'docids': formatted_doc_ids}\n\n# Example usage\n\nif __name__ == \"__main__\":\n    # Paths to the TF-IDF vector files\n    corpus_path = '/kaggle/input/dis-tokens/corpus_tokens.json'\n    queries_path = '/kaggle/input/dis-tokens/test_tokens.json'\n    doc_tf_idf_path = '/kaggle/input/tf-idf/doc_tf_idf_vectors.json'\n    query_tf_idf_path = '/kaggle/input/tf-idf/query_tf_idf_vectors.json'\n    index_path = '/kaggle/input/tf-idf/inverted_index.json'\n    term_to_index_path = '/kaggle/input/tf-idf/vocab.json'\n\n    # Step 1: Calculate IDF using the entire corpus\n    # Uncomment if running for the first time\n    # idf_dict, term_to_index = calculate_idf_from_corpus(corpus_path, term_to_index_path)\n\n    # Step 2: Compute TF-IDF vectors for all documents\n    # Uncomment if running for the first time\n    # compute_tf_idf_for_documents(corpus_path, idf_dict, term_to_index, doc_tf_idf_path)\n\n    # Step 3: Compute TF-IDF vectors for all queries\n    # Uncomment if running for the first time\n    # compute_tf_idf_for_queries(queries_path, idf_dict, term_to_index, query_tf_idf_path)\n\n    # Step 4: Build the inverted index and save it to disk incrementally\n    # Uncomment if running for the first time\n    # build_inverted_index_on_disk(doc_tf_idf_path, index_path)\n\n    # Step 5: Retrieve top documents using the optimized inverted index with limited candidates\n    submission_data = retrieve_top_documents_with_limited_candidates(\n        query_tf_idf_path,\n        doc_tf_idf_path,\n        index_path,\n        term_to_index_path,\n        top_n=10,\n        K=5,   # Top K terms per query\n        M=200  # Top M documents per term\n    )\n\n    # Step 6: Convert to DataFrame and save to CSV\n    submission_df = pd.DataFrame(submission_data)\n    submission_df = submission_df[['id', 'docids']]  # Ensure correct column order\n\n    # Verify that there are 2000 lines plus header\n    assert len(submission_df) == 2000, f\"Expected 2000 queries, but got {len(submission_df)}\"\n\n    submission_file_path = 'submission.csv'\n    submission_df.to_csv(submission_file_path, index=False)\n\n    log(f\"Submission file '{submission_file_path}' created successfully.\")\n    log(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T20:02:03.960164Z","iopub.execute_input":"2024-11-09T20:02:03.960604Z","iopub.status.idle":"2024-11-09T20:04:15.950473Z","shell.execute_reply.started":"2024-11-09T20:02:03.960563Z","shell.execute_reply":"2024-11-09T20:04:15.948423Z"}},"outputs":[{"name":"stdout","text":"2024-11-09 20:02:04 Starting retrieval of top documents for each query...\n2024-11-09 20:02:23 Term to index mapping loaded.\n2024-11-09 20:02:23 Query vectors loaded.\n2024-11-09 20:02:23 Term to queries mapping for top terms created.\n2024-11-09 20:03:13 Processed 1000000 terms from the inverted index.\n2024-11-09 20:03:21 Processed 2000000 terms from the inverted index.\n2024-11-09 20:03:28 Processed 3000000 terms from the inverted index.\n2024-11-09 20:03:35 Processed 4000000 terms from the inverted index.\n2024-11-09 20:03:43 Processed 5000000 terms from the inverted index.\n2024-11-09 20:03:49 Processed 6000000 terms from the inverted index.\n2024-11-09 20:03:56 Processed 7000000 terms from the inverted index.\n2024-11-09 20:04:05 Processed 8000000 terms from the inverted index.\n2024-11-09 20:04:11 Processed 9000000 terms from the inverted index.\n","output_type":"stream"},{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}